单变量线性回归算法→多元线性回归算法→二分类算法

拟合函数方程→（损失函数）→成本函数→梯度下降算法→学习率



## 梯度下降算法

梯度下降算法，当函数不只有一个极小值时，设置的初始值不同，找到的极小值不同

![微信图片_20250819153149](C:\Users\User\Desktop\自学AI笔记\微信图片_20250819153149.png)

## 学习率原理（梯度下降直觉）

学习率会随着斜率的下降步长变得更小。

![微信图片_20250819153153](C:\Users\User\Desktop\自学AI笔记\微信图片_20250819153153.png)

### github搜索关键词：

![微信图片_20250819153108](C:\Users\User\Desktop\自学AI笔记\微信图片_20250819153108.png)

size:>50是大于50kb的代码文件

## 将多特征向量化后点乘的优势：

![image-20250819112948187](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819112948187.png)

这是由于我们使用了Numpy库进行计算，而Numpy库并行使用电脑硬件的速度比程序执行三大结构的速度更快。

![image-20250819114551500](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819114551500.png)

### 单一线性回归与多元线性回归的参数更新对比：

![image-20250819144902623](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819144902623.png)

### **正规方程**

**好处是：可以直接用Numpy解出w,b的值而不用迭代，坏处是：只能用于线性回归，不能用于其他的学习算法（逻辑回归算法、神经网络算法等），并且当特征值非常大的时候（>10,000）就会计算的很慢。**

## 特征缩放

![image-20250819150907578](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819150907578.png)

为提高梯度下降速度，建议无论是不是一个区间的不同特征的原始数据，都进行归一化处理。

归一化处理有三种方式：最大值归一化、均值归一化、

Z-score归一化（标准差归一化）

![image-20250819152817464](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819152817464.png)

## 多元线性回归的学习率选择

![image-20250819153904279](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819153904279.png)

如果代码的成本函数像左上角的图一样来回波动，可以先改小学习率α（三倍、1/3倍），如果阿尔法非常小了，J还是不行，那该改代码了。



Scikit-learn

## 逻辑回归算法（即二分类算法）

### sigmord函数：

![image-20250819165359439](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819165359439.png)

### 回归算法都变了，成本函数若还是平方差成本函数就会导致：

![image-20250819165125160](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819165125160.png)

会有许多极小值，收敛不到全局最小。

**在二分类算法中，提出了新的损失函数的概念，定义了新的成本函数公式（起源于概率论的最大似然），随后为求得最小误差的预测函数，依然需要梯度下降法进行求min。**

![image-20250819201705527](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819201705527.png)

**诶？这偏导和线性回归不是一样的！？不不，f不一样啦**

![image-20250819202836294](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819202836294.png)

**欠拟合（高偏差）、刚好、过拟合（高方差）**

![image-20250819203955109](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819203955109.png)

二分类也有这个问题，就不放图了。

解决过拟合的办法：三个

![image-20250819205717915](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819205717915.png)

正则化：

![image-20250819213303476](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250819213303476.png)

加上wj的意义是，让成本函数在找最低偏差的同时保证wj都是很小的数值，因为太大容易过拟合。其中λ>0，且不能过大或过小。

## 恭喜你完成机器学习部分了（监督学习）

# 深度学习部分

**需求预测：比如预测一个产品是否会成为畅销品，其实质上是逻辑回归算法**

![image-20250911210626636](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250911210626636.png)

**神经网络不需要手动设计特征，它可以自己学习创造特征，进行由隐藏层到结果的逻辑回归算法。**

**我们需要做的就是设计神经网络架构：即设计多少个隐藏层？每个隐藏层里有多少个神经元？**

## 神经网络层

### 理论部分：以逻辑回归问题为例

![image-20250917212637791](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250917212637791.png)

**g是激活函数，这里采用逻辑回归算法（sigmoid函数），还可以使用别的函数作为激活函数。**

### 代码部分：在TensorFlow中实现

![image-20250917213219802](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250917213219802.png)

**首先需要了解在numpy里如何存储向量和矩阵：**

![image-20250917213742006](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250917213742006.png)

![image-20250917214426551](C:\Users\User\AppData\Roaming\Typora\typora-user-images\image-20250917214426551.png)
